# ì›¹ì‚¬ì´íŠ¸ì˜ HTMLì„ ì§ì ‘ ë¶„ì„í•˜ëŠ” ì¼ë°˜ ì›¹ í¬ë¡¤ëŸ¬ selenium ì‚¬ìš©.
# íŠ¹ì • íƒœê·¸(XPath)ì—ì„œ ë‰´ìŠ¤ url, ë³¸ë¬¸ ì¶”ì¶œ.

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import time
import random

def crawl_bbc_most_read():
    """BBC ë‰´ìŠ¤ Most Read ì„¹ì…˜ì—ì„œ ìƒìœ„ 10ê°œ ê¸°ì‚¬ í¬ë¡¤ë§"""
    print("í¬ë¡¤ëŸ¬ ì‹œì‘ë¨")
    
    # Chrome ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜µì…˜ ì„¤ì •
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36")

    # WebDriver ì‹¤í–‰
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # BBC Most Read í˜ì´ì§€ ì—´ê¸°
    url = "https://www.bbc.com/news"
    driver.get(url)
    time.sleep(random.uniform(3, 6))  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ëœë¤ ë”œë ˆì´)

    print("ğŸ“Œ í˜„ì¬ URL:", driver.current_url)
    print("ğŸ“Œ í˜„ì¬ í˜ì´ì§€ ì œëª©:", driver.title)
    
    # Most Read ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
    try:
        # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, "//*[@id='main-content']/article/section[6]/div/div[2]/div/div/a/div/div/div/h2")))
        
        articles = driver.find_elements(By.XPATH, "//*[@id='main-content']/article/section[6]/div/div[2]/div/div/a")
        article_links = []

        for article in articles:
            link = article.get_attribute("href")
            if link and "https://www.bbc.com/news" in link:
                article_links.append(link)

        article_links = list(set(article_links))[:10]  # ì¤‘ë³µ ì œê±° í›„ ìƒìœ„ 10ê°œ ì„ íƒ
        print(f"âœ… Most Read ê¸°ì‚¬ {len(article_links)}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ XPath ì—ëŸ¬: {e}")
        driver.quit()
        return None
    
    # ì—¬ëŸ¬ ê°œì˜ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±
    articles_data =[]

    # ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
    for article_url in article_links:
        driver.get(article_url)
        time.sleep(random.uniform(3, 6))  # ëœë¤ ë”œë ˆì´ ì¶”ê°€

        try:
            paragraphs = driver.find_elements(By.XPATH, "//article//p")
            content = "\n".join([p.text for p in paragraphs if p.text.strip()])

            print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {article_url}")
            print(f"âœ… í¬ë¡¤ë§ëœ ë³¸ë¬¸: {content[:100]}...")

            # í¬ë¡¤ë§í•œ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            articles_data.append((article_url, content))
            
        except Exception as e:
            print(f"âŒ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    driver.quit()
    
    return articles_data # ì—¬ëŸ¬ ê°œì˜ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë°˜í™˜